---
title: "Analysis for x linguistic processing"
output: html_notebook
---

```{r}
shhh <- suppressPackageStartupMessages # It's a library, so shhh!

shhh(library( mgcv ))
shhh(library(dplyr))
shhh(library(ggplot2))
shhh(library(lme4))
shhh(library(tidymv))
shhh(library(gamlss))
shhh(library(gsubfn))
shhh(library(lmerTest))
shhh(library(tidyverse))
shhh(library(boot))
shhh(library(rsample))
shhh(library(plotrix))
shhh(library(ggrepel))
shhh(library(mgcv))

theme_set(theme_bw())
options(digits=4)
options(dplyr.summarise.inform = FALSE)
```

```{r}

set.seed(444)

```

## Compute DLL for Each Language

```{r}

model_cross_val = function(form, df, d_var, num_folds=10){
  
  folds <- cut(seq(1,nrow(df)),breaks=num_folds,labels=FALSE)
  
  estimates <- c()
  models <- c()
  for(i in 1:num_folds){
    testIndexes = which(folds==i,arr.ind=TRUE)
    testData = df[testIndexes,]
    trainData = df[-testIndexes,]

    model = lm(as.formula(form), data = trainData)

    stdev = sigma(model)
    densities <- log(dnorm(testData[[d_var]],
                          mean=predict(model, newdata=testData),
                          sd=stdev))

    estimates <- c(estimates, densities)
  }

  return(estimates)
}

```


## Baseline Investigation of Surprisal

Compare model with surprisal on w, w - 1, and w -2 to models where surprisal has been dropped for each slot.

```{r}
langs = c("du", "en", "fi", "ge", "gr", "he", "it", "ko", "sp", "tr", "ru")
contexts = c("short", "long")

regression_names = c("bl", "0", "1", "2")
# "bl" = baseline model with full surprisals, 0 = surprisal dropped at slot 0 i.e. the current word

regression_forms = c(
  "psychometric ~ surp + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len",
  "psychometric ~ prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len",
  "psychometric ~ surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len",
  "psychometric ~ surp + prev_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"
  )


dll_xlang_surp_df = data.frame()
for (lang in langs) {

  print(paste0("Fitting model for ", lang))
  
  df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>%
    filter(context == "long") %>%
    mutate(psychometric = as.double(psychometric))
    
  loglik_df = data.frame(names=regression_names, forms=regression_forms) %>%
    mutate(logliks = map(regression_forms, model_cross_val, df=df, d_var="psychometric" )) %>%
    dplyr::select(-forms)
  
  dlls = list()
  
  dll_df = data.frame()
  for (i in regression_names){
      ll1 = loglik_df[loglik_df["names"] == "bl"][2][[1]]
      ll2 = loglik_df[loglik_df["names"] == i][2][[1]]
      dll = ll1 - ll2
      dll = dll[!is.na(dll)]
      ttest = t.test(dll)
      dll_df = rbind(dll_df, data.frame(comp = i, mean = mean(dll), 
                                        upper = mean(dll) + (1.96 * std.error(dll)), lower = mean(dll) - (1.96 * std.error(dll)),
                                        ttest_pval = ttest$p.value, ttest_est = ttest$estimate))
  }
  dll_xlang_surp_df = rbind(dll_xlang_surp_df, dll_df %>% mutate(lang = lang))
  
}

```

Plotting for these results

```{r}

dll_xlang_surp_df %>%
  mutate(comp = factor(comp, levels = c("2", "1", "0"))) %>%
  rename(target = comp) %>%
  filter(target != "bl") %>%
  mutate(sig = if_else(ttest_pval < 0.05, "p<0.05", "N.S.")) %>%
  ggplot(aes(x = target, y = mean, color = target)) +
    geom_hline(yintercept=0, color="blue", linetype="dashed", alpha =0.5) +
    geom_point(position = position_dodge(width = 0.6)) +
    #geom_text(aes(y = -0.0025, label = sig), color = "black", size = 2) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.1, position = position_dodge(width = 0.6)) +
    ylab("ΔLogLiklihood (per word)") + 
    xlab("") +
    scale_x_discrete(labels = c(bquote(w[t-2]), bquote(w[t-1]), bquote(w[t]))) +
    ggtitle("Contribution of Surprisal to ΔLL") +
    facet_wrap(~lang, nrow = 2) +
  theme(
    legend.position = "none"
  )

ggsave("./images/xlang_dll_surp.png", width = 8, height = 4.5)


```

## Replace Surprisal w/ Entropy

Here "bl" = baseline with only surprisal and "0" = model where surprisal has been replaced with entropy at the current word

```{r }
langs = c("du", "en", "fi", "ge", "gr", "he", "it", "ko", "sp", "tr", "ru")

regression_names = c("bl", "0", "1", "2")

regression_forms = c(
  "psychometric ~ surp + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len",
  "psychometric ~ ent + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len",
  "psychometric ~ surp + prev_ent + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len",
  "psychometric ~ surp + prev_surp + prev2_ent + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"
  )

dll_xlang_ent_df = data.frame()
for (lang in langs) {

  print(paste0("Fitting model for ", lang))
  df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>%
    filter(context == "long") %>%
  mutate(psychometric = as.double(psychometric)) %>%
  mutate(psychometric = if_else(is.na(psychometric), 0, psychometric)) %>%
  group_by(itemid, ianum, ia_text, surp, prev_surp, prev2_surp, len, prev_len, prev2_len, freq, prev_freq, prev2_freq, ent, prev_ent, prev2_ent, context) %>%
    summarise(psychometric = mean(psychometric)) %>%
  ungroup()
  
  loglik_df = data.frame(names=regression_names, forms=regression_forms) %>%
    mutate(logliks = map(regression_forms, model_cross_val, df=df, d_var="psychometric" )) %>%
    dplyr::select(-forms)
  
  dlls = list()
  
  dll_df = data.frame()
  for (i in regression_names){
      ll1 = loglik_df[loglik_df["names"] == "bl"][2][[1]]
      ll2 = loglik_df[loglik_df["names"] == i][2][[1]]
      dll = ll1 - ll2
      dll = dll[!is.na(dll)]
      ttest = t.test(dll)
      dll_df = rbind(dll_df, data.frame(comp = i, mean = mean(dll), 
                                        upper = mean(dll) + (1.96 * std.error(dll)), lower = mean(dll) - (1.96 * std.error(dll)),
                                        ttest_pval = ttest$p.value, ttest_est = ttest$estimate))
  }
  dll_xlang_ent_df = rbind(dll_xlang_ent_df, dll_df %>% mutate(lang = lang))
  
}

```

Plotting for these results

```{r}

dll_xlang_ent_df %>%
  mutate(comp = factor(comp, levels = c("2", "1", "0"))) %>%
  mutate(mean = -mean, upper = -upper, lower = -lower) %>%
  mutate(comp = case_when(comp == "0" ~ "w0", comp == "1" ~ "w-1", comp == "2" ~ "w-2")) %>%
  rename(target = comp) %>%
  filter(target != "bl") %>%
  mutate(sig = if_else(ttest_pval < 0.05, "p<0.05", "N.S.")) %>%
  ggplot(aes(x = target, y = mean, color = target)) +
    geom_hline(yintercept=0, color="blue", linetype="dashed", alpha =0.5) +
    geom_point(position = position_dodge(width = 0.6)) +
    #geom_text(aes(y = -0.0025, label = sig), color = "black", size = 2) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.1, position = position_dodge(width = 0.6)) +
    ylab("ΔLogLiklihood (per word)") + 
    xlab("Language") +
    scale_x_discrete(labels = c(bquote(w[t-2]), bquote(w[t-1]), bquote(w[t]))) +
    ggtitle("Replacing Surprisal w/ Entropy") +
    facet_wrap(~lang, nrow = 3) +
  theme(
    legend.position = "none"
  )

ggsave("./images/xlang_dll_ent_replace.png", width = 8, height = 5)


```

## Add Entropy

Compare baseline model to models where entropy has been added as an additional predictor for each slot. "bl" = baseline with just surprisal "0" = model where entropy has been added at the current word.

```{r}
langs = c("du", "en", "fi", "ge", "gr", "he", "it", "ko", "sp", "tr", "ru")

regression_names = c("bl", "0", "1", "2")

regression_forms = c(
  "psychometric ~ surp + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len",
  "psychometric ~ surp + ent + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len",
  "psychometric ~ surp + prev_surp + prev_ent + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len",
  "psychometric ~ surp + prev_surp + prev2_surp + prev2_ent + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"
  )

dll_xlang_add_df = data.frame()
for (lang in langs) {

  print(paste0("Fitting model for ", lang))
  df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>%
    filter(context == "long") %>%
    mutate(psychometric = as.double(psychometric)) %>%
    mutate(psychometric = if_else(is.na(psychometric), 0, psychometric)) %>%
    group_by(itemid, ianum, ia_text, surp, prev_surp, prev2_surp, len, prev_len, prev2_len, freq, prev_freq, prev2_freq, ent, prev_ent, prev2_ent, context) %>%
      summarise(psychometric = mean(psychometric)) %>%
    ungroup()
  
  loglik_df = data.frame(names=regression_names, forms=regression_forms) %>%
    mutate(logliks = map(regression_forms, model_cross_val, df=df, d_var="psychometric" )) %>%
    dplyr::select(-forms)
  
  dlls = list()
  
  dll_df = data.frame()
  for (i in regression_names){
      ll1 = loglik_df[loglik_df["names"] == "bl"][2][[1]]
      ll2 = loglik_df[loglik_df["names"] == i][2][[1]]
      dll = ll1 - ll2
      dll = dll[!is.na(dll)]
      ttest = t.test(dll)
      dll_df = rbind(dll_df, data.frame(comp = i, mean = mean(dll), 
                                        upper = mean(dll) + (1.96 * std.error(dll)), lower = mean(dll) - (1.96 * std.error(dll)),
                                        ttest_pval = ttest$p.value, ttest_est = ttest$estimate))
  }
  dll_xlang_add_df = rbind(dll_xlang_add_df, dll_df %>% mutate(lang = lang))
  
}

```

```{r}

dll_xlang_add_df %>%
  mutate(comp = factor(comp, levels = c("2", "1", "0"))) %>%
  mutate(mean = -mean, upper = -upper, lower = -lower) %>%
  mutate(comp = case_when(comp == "0" ~ "w0", comp == "1" ~ "w-1", comp == "2" ~ "w-2")) %>%
  rename(target = comp) %>%
  filter(target != "bl") %>%
  mutate(sig = if_else(ttest_pval < 0.05, "p<0.05", "N.S.")) %>%
  ggplot(aes(x = target, y = mean, color = target)) +
    geom_hline(yintercept=0, color="blue", linetype="dashed", alpha =0.5) +
    geom_point(position = position_dodge(width = 0.6)) +
    #geom_text(aes(y = -0.0025, label = sig), color = "black", size = 2) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.1, position = position_dodge(width = 0.6)) +
    ylab("ΔLogLiklihood (per word)") + 
    xlab("Language") +
    scale_x_discrete(labels = c(bquote(w[t-2]), bquote(w[t-1]), bquote(w[t]))) +
    ggtitle("Adding Entropy") +
    facet_wrap(~lang, nrow = 3) +
  theme(
    legend.position = "none"
  )

ggsave("./images/xlang_dll_ent_add.png", width = 8, height = 5)


```




## Compare DLL to Linguistic Exposure

```{r}

lang_exposure = data.frame( lang = c("du", "en", "fi", "ge", "gr", "he", "it", "ko", "ru", "sp", "tr"  ),
                            ntoks = c(15.96, 41.21, 2.99, 40.37, 3.61, 0.69, 32.94, 6.28, 39.39, 24.45, 22.52 ))
dll_df = dll_xlang_surp_df %>%
  merge(lang_exposure, by = "lang") %>%
  filter(comp == "0")

cor.test(dll_df$mean, dll_df$ntoks)

dll_df %>%
  ggplot(aes(x=ntoks, y = mean, label = lang)) +
  geom_smooth(method = "lm") +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.3) +
  geom_label_repel() +
  ylab("ΔLogLiklihood (per word)") + 
  xlab("# of Training Tokens (Billion)") +
  ggtitle("# Training Tokens vs. ΔLL")

ggsave("./images/dll_vs_training-size.png", width = 5.5, height = 4)


```

## Investigating Correlations in WALS

This code block looks at a few features from the WALS dataset, comparing these properties of a language to its DLL (for surprisal)

```{r}
lang_code = c("dut"="du", "eng"="en", "fin"="fi", "ger"="ge", "grk"="gr", "heb"="he", "ita"="it", "kor"="ko", "rus"="ru", "spa"="sp", "tur"="tr" )
langs = c("du", "en", "fi", "ge", "gr", "he", "it", "ko", "sp", "tr", "ru")


wals_df = read.csv("../data/wals.csv") %>%
  mutate(lang = lang_code[wals_code]) %>%
  filter(lang %in% langs)
  
struct_df = dll_xlang_surp_df %>%
  merge(wals_df, by = "lang") %>%
  filter(comp == "0")

# Does the model have better predictive power for indo-european languages? (Results suggest no)

struct_df %>%
  dplyr::select(lang, mean, lower, upper, starts_with("family")) %>%
  rename(pred = 5) %>%
  ggplot(aes(x = reorder(pred, mean), y = mean, color = lang)) +
    geom_point(position = position_dodge(width = 0.6)) +
    #geom_text(aes(y = -0.0025, label = sig), color = "black", size = 2) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.1, position = position_dodge(width = 0.6))

# Does work order affect model predictive power? (Agagin, no)

struct_df %>%
  dplyr::select(lang, mean, lower, upper, starts_with("X81A")) %>%
  rename(pred = 5) %>%
  ggplot(aes(x = reorder(pred, mean), y = mean, color = lang)) +
    geom_point(position = position_dodge(width = 0.6)) +
    #geom_text(aes(y = -0.0025, label = sig), color = "black", size = 2) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.1, position = position_dodge(width = 0.6))

# What about the relationship between DLL and average orthographic word length of a language?
# There does seem to be a slight correlation -- probably langauges with longer words are being read more slowly on average

word_len_lang = data.frame()
for (lang in langs){
  len_df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>%
    filter(context == "long") %>%
    summarise(len = mean(len))
  word_len_lang = rbind(word_len_lang, len_df %>% mutate(lang = lang))
}

dll_xlang_surp_df %>%
  filter(lang != "ko") %>%
  filter(comp == "0") %>%
  merge(word_len_lang, by = "lang")  %>%
  ggplot(aes(x = len, y = mean)) +
    geom_smooth(method = "lm") +
    geom_point() +
    geom_label_repel(aes(label = lang)) +
  xlab("Mean word length (orthographic)") +
  ylab("Delta Log Lik (surprisal)")
ggsave("./dll_wordLen.png", width = 5, height = 4)

dll_len_df = dll_xlang_surp_df %>%
  filter(lang != "ko") %>%
  filter(comp == "0") %>%
  merge(word_len_lang, by = "lang")

cor.test(dll_len_df$len, dll_len_df$mean)

```


## Shape of surprisal / RT relationship

```{r}

fit_gam_inner = function(bootstrap_sample, mean_predictors, is_linear) {
  
  df = bootstrap_sample$data
  weights = tabulate(as.integer(bootstrap_sample), nrow(df))
  
  if (is_linear) {
    m = gam(psychometric ~ surp + prev_surp + te(freq, len, bs = 'cr') + te(prev_freq, prev_len, bs = 'cr'), data = df, weights = weights)
    terms_to_predict = c("surp", "prev_surp")
  } else {
    m = gam(psychometric ~ s(surp, bs = 'cr', k = 6) + s(prev_surp, bs = 'cr', k = 6) + te(freq, len, bs = 'cr') + te(prev_freq, prev_len, bs = 'cr'), data = df, weights = weights)
    terms_to_predict = c("s(surp)", "s(prev_surp)")

  }
  
  # For fitting with smooths for by-subject effects
  # m = gam(psychometric ~ s(surp, bs='tp', k=6) + s(uniform_id, surp, bs='fs', m=1, k=6) + te(freq, len) + s(prev_surp, bs='tp', k=6) + s(uniform_id, prev_surp, bs='fs', m=1, k=6) + te(prev_freq, prev_len), data = df, weights = weights)

  newdata = data.frame(surp=seq(0,20,by=0.1),
                       prev_surp=mean_predictors$surp,
                       freq=mean_predictors$freq, prev_freq=mean_predictors$freq,
                       len=mean_predictors$freq, prev_len=mean_predictors$freq) 
  
  # Returns a matrix N_samples * N_terms.
  per_term_predictions = predict(m, newdata=newdata, terms=terms_to_predict, type="terms")

  # Additive model -- sum across predictor response contributions (matrix columns).
  predictions = rowSums(per_term_predictions)

  return(newdata %>% mutate(y=predictions))
}

fit_gam = function(df, mean_predictors, is_linear, alpha=0.05) {
  # Bootstrap-resample data
  boot_models = df %>% bootstraps(times=10) %>% 
   # Fit a GAM and get predictions for each sample
    mutate(smoothed=map(splits, fit_gam_inner, mean_predictors=mean_predictors, is_linear = is_linear))
  
  # Extract mean and 5% and 95% percentile y-values for each surprisal value
  result = boot_models %>% 
    unnest(smoothed) %>% 
    dplyr::select(surp, y) %>% 
    group_by(surp) %>% 
      summarise(y_lower=quantile(y, alpha / 2), 
                y_upper=quantile(y, 1 - alpha / 2),
                y=mean(y)) %>% 
    ungroup()
  
  return (result)
}

```


Get linear + non-linear gam smooths for each of our languages

```{r}

langs = c("du", "en", "fi", "ge", "gr", "he", "it", "ko", "sp", "tr", "ru")


xlang_linear_smooths_long = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>% filter(context == "long") %>%
  mean_predictors = merged_df %>% summarise(surp = mean(surp), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=T)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_linear_smooths_long = rbind(xlang_linear_smooths_long, gam_smooths %>% mutate(lang = lang, context = "long", linear = "linear"))
}

xlang_nonlinear_smooths_long = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>% filter(context == "long") %>%
  mean_predictors = merged_df %>% summarise(surp = mean(surp), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=F)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_nonlinear_smooths_long = rbind(xlang_nonlinear_smooths_long, gam_smooths %>% mutate(lang = lang, context = "long", linear = "non-linear" ))
}

xlang_linear_smooths_short = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>% filter(context == "short") %>%
  mean_predictors = merged_df %>% summarise(surp = mean(surp), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=T)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_linear_smooths_short = rbind(xlang_linear_smooths_short, gam_smooths %>% mutate(lang = lang, context = "short", linear = "linear"))
}

xlang_nonlinear_smooths_short = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>% filter(context == "short") %>%
  mean_predictors = merged_df %>% summarise(surp = mean(surp), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=F)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_nonlinear_smooths_short = rbind(xlang_nonlinear_smooths_short, gam_smooths %>% mutate(lang = lang, context = "short", linear = "non-linear"))
}

```

```{r}

write.csv(xlang_nonlinear_smooths_short, "./gam_saves/nonlinear_short.csv")
write.csv(xlang_linear_smooths_short, "./gam_saves/linear_short.csv")
write.csv(xlang_nonlinear_smooths_long, "./gam_saves/nonlinear_long.csv")
write.csv(xlang_linear_smooths_long, "./gam_saves/linear_long.csv")

```

## Density Data

```{r}

get_d_points = function(df) {
    x = density(df$surp)$x
    y = density(df$surp)$y
    return(data.frame(x, y))
  }

density_data_short = data.frame()
for(lang in langs) {
  merged_df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>% filter(context == "short")
  density_data = merged_df %>%
    do({get_d_points(.)}) %>%
    filter(x>0, x<20)
  density_data_short = rbind(density_data_short, density_data %>% mutate(lang = lang, context = "short"))
}

density_data_long = data.frame()
for(lang in langs) {
  merged_df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>% filter(context == "long")
  density_data = merged_df %>%
    do({get_d_points(.)}) %>%
    filter(x>0, x<20)
density_data_long = rbind(density_data_long, density_data %>% mutate(lang = lang, context = "long"))

}
 

```

## Plot Surprisal / RT relationship for Short & Long Contexts

```{r}

# Surprisal curves for long context
  ggplot() +
      annotate("rect", xmin=0, xmax=20, ymin=-18,ymax=-13, fill="blue", alpha=0.05) +
      geom_line(data = density_data_long, aes(x=x, y=y*50 - 18), color="grey") +
      geom_line(data = xlang_linear_smooths_long, aes(x=surp, y=y, color = linear), size=0.5) +
      geom_line(data = xlang_nonlinear_smooths_long, aes(x=surp, y=y, color = linear), size=0.5) +
      geom_ribbon(data = xlang_nonlinear_smooths_long, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      geom_ribbon(data = xlang_linear_smooths_long, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      scale_x_continuous(labels=c(0, 10, 20), breaks=c(0, 10, 20), minor_breaks = NULL) +
      facet_wrap(~lang, nrow = 2) +
      ylab("Slowdown due to surprisal (ms)") +
      xlab("Surprisal of Word") +
      ggtitle("Effect of Surprisal on Reading Time across Languages \n Long Context Window")
  
  ggsave("./images/surp_mgpt_long.png", height = 6, width = 8)


```


```{r}

# Surprisal curves for short context
  ggplot() +
      annotate("rect", xmin=0, xmax=20, ymin=-18,ymax=-13, fill="blue", alpha=0.05) +
      geom_line(data = density_data_short, aes(x=x, y=y*50 - 18), color="grey") +
      geom_line(data = xlang_linear_smooths_short, aes(x=surp, y=y, color = linear), size=0.5) +
      geom_line(data = xlang_nonlinear_smooths_short, aes(x=surp, y=y, color = linear), size=0.5) +
      geom_ribbon(data = xlang_nonlinear_smooths_short, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      geom_ribbon(data = xlang_linear_smooths_short, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      scale_x_continuous(labels=c(0, 10, 20), breaks=c(0, 10, 20), minor_breaks = NULL) +
      facet_wrap(~lang, nrow = 2) +
      ylab("Slowdown due to surprisal (ms)") +
      xlab("Surprisal of Word") +
      ggtitle("Effect of Surprisal on Reading Time across Languages \nShort Context Window")
  
  ggsave("./images/surp_mgpt_short.png", height = 6, width = 8)

```


## Hoover et al. (2022) "Superlinearity" Metric

```{r}


contexts = c("short", "long")
langs = c("du", "en", "fi", "ge", "gr", "he", "it", "ko", "sp", "tr", "ru")

non_linear_delta_df = data.frame()

for(lang in langs) {
for(ctext in contexts) {
  
  df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>% filter(context == ctext)
  
  median_surp = median(df$surp)
  
  high_surp = df %>% filter(surp >= median_surp)
  low_surp = df %>% filter(surp < median_surp)
  
  high_lm = lm(psychometric ~ surp + len + freq + prev_surp + prev_len + prev_freq, data =  high_surp)
  b_high = high_lm$coefficients[2]

  summary(high_lm)
  low_lm = lm(psychometric ~ surp + len + freq + prev_surp + prev_len + prev_freq, data =  low_surp)
  b_low = low_lm$coefficients[2]
  
  delta = b_high - b_low
  delta_df = data_frame(delta = delta, lang = lang, context=ctext)
  
  non_linear_delta_df = rbind(non_linear_delta_df, delta_df)
  
}
}


```

Plot the superlinearity metric by context

```{r}
non_linear_delta_df %>%
  ggplot(aes(x = lang, y = delta, fill = context)) +
  geom_bar(stat="identity", position = position_dodge(width = 0.9)) +
  facet_grid(context~.) +
  ggtitle("Superlinearity (from Hoover et al.) by language") +
  ylab("Superlinearity")
  xlab("Language")
  
ggsave("./superlinearity.png", width = 6, height = 3)


```


