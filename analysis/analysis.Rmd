---
title: "Analysis for x linguistic processing"
output: html_notebook
---

```{r}
shhh <- suppressPackageStartupMessages # It's a library, so shhh!

shhh(library( mgcv ))
shhh(library(dplyr))
shhh(library(ggplot2))
shhh(library(lme4))
shhh(library(tidymv))
shhh(library(gamlss))
shhh(library(gsubfn))
shhh(library(lmerTest))
shhh(library(tidyverse))
shhh(library(boot))
shhh(library(rsample))
shhh(library(plotrix))
shhh(library(ggrepel))
shhh(library(mgcv))

theme_set(theme_bw())
options(digits=4)
options(dplyr.summarise.inform = FALSE)
```

```{r}

set.seed(444)
langs = c("du", "en", "fi", "ge", "gr", "he", "it", "ko", "sp", "tr", "ru")
psychometrics = c("total_rt", "gaze_rt", "firstfix_rt")
contexts = c("short", "long")

```

## Compute DLL for Each Language

```{r}

model_cross_val = function(form, df, d_var, num_folds=10){
  
  folds <- cut(seq(1,nrow(df)),breaks=num_folds,labels=FALSE)
  
  estimates <- c()
  models <- c()
  for(i in 1:num_folds){
    testIndexes = which(folds==i,arr.ind=TRUE)
    testData = df[testIndexes,]
    trainData = df[-testIndexes,]

    model = lm(as.formula(form), data = trainData)

    stdev = sigma(model)
    densities <- log(dnorm(testData[[d_var]],
                          mean=predict(model, newdata=testData),
                          sd=stdev))

    estimates <- c(estimates, densities)
  }

  return(estimates)
}

```


## Baseline Investigation of Surprisal

Compare model with surprisal on w, w - 1, and w -2 to models where surprisal has been dropped for each slot.

```{r}

regression_names = c("bl", "0", "1", "2")
# "bl" = baseline model with full surprisals, 0 = surprisal dropped at slot 0 i.e. the current word


dll_xlang_surp_df = data.frame()
for (lang in langs) {

  print(paste0("Fitting model for ", lang))
  
  df = read.csv(paste0("./cleaned_data/l1/", lang, "_clean_data.csv")) %>%
    filter(context == "long")
  
  for (psychometric in psychometrics) {
    
    regression_forms = c(
      paste0(psychometric, " ~ surp + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ surp + prev_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len")
    )
    
    loglik_df = data.frame(names=regression_names, forms=regression_forms) %>%
      mutate(logliks = map(regression_forms, model_cross_val, df=df, d_var=psychometric )) %>%
      dplyr::select(-forms)
    
    dlls = list()
    
    dll_df = data.frame()
    for (i in regression_names){
        ll1 = loglik_df[loglik_df["names"] == "bl"][2][[1]]
        ll2 = loglik_df[loglik_df["names"] == i][2][[1]]
        dll = ll1 - ll2
        dll = dll[!is.na(dll)]
        ttest = t.test(dll)
        dll_df = rbind(dll_df, data.frame(comp = i, mean = mean(dll), 
                                          upper = mean(dll) + (1.96 * std.error(dll)), lower = mean(dll) - (1.96 * std.error(dll)),
                                          ttest_pval = ttest$p.value, ttest_est = ttest$estimate))
    }
    dll_xlang_surp_df = rbind(dll_xlang_surp_df, dll_df %>% mutate(lang = lang, psychometric = psychometric))
  }
}

```

Plotting for these results


```{r}

options(scipen=999)


dll_xlang_surp_df %>%
  mutate(comp = factor(comp, levels = c("2", "1", "0"))) %>%
  rename(target = comp) %>%
  filter(target != "bl") %>%
  mutate(sig = case_when( ttest_pval >= 0.05 & ttest_est > 0 ~ " ",
                          ttest_pval < 0.05 & ttest_pval >= 0.01 & ttest_est > 0 ~ "*",
                          ttest_pval < 0.01 & ttest_pval >= 0.001 & ttest_est > 0 ~ "**",
                          ttest_pval < 0.001 & ttest_est > 0 ~ "***")) %>%
  mutate(psychometric = case_when(psychometric == "firstfix_rt" ~ "First Fixation", 
                                  psychometric == "gaze_rt" ~ "Gaze Duration", 
                                  psychometric == "total_rt" ~ "Total Fixation")) %>%
  ggplot(aes(x = target, y = mean, color = target)) +
    geom_hline(yintercept=0, color="black", linetype="dashed", alpha =0.5) +
    geom_point(position = position_dodge(width = 0.6)) +
    geom_text(aes(y = 0.08, label = sig, color = target), size = 3) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.1, position = position_dodge(width = 0.6)) +
    ylab("ΔLogLiklihood (per word)") + 
    xlab("") +
    facet_grid(psychometric~lang) +
    scale_x_discrete(labels = c(bquote(w[t-2]), bquote(w[t-1]), bquote(w[t]))) +
    scale_color_manual(values = c("#a1dab4", "#41b6c4", "#225ea8")) +
    #ggtitle("Contribution of Surprisal to ΔLL") +
  theme(
    legend.position = "none",
    axis.title.x = element_blank()#,
    #panel.border = element_rect(color = "grey", fill = NA, size = 0.5)
  )

ggsave("./images/l1/dll_surp.png", width = 9.2, height = 4)


```

## Replace Surprisal w/ Entropy

Here "bl" = baseline with only surprisal and "0" = model where surprisal has been replaced with entropy at the current word

```{r }


regression_names = c("bl", "0", "1", "2")

dll_xlang_ent_df = data.frame()
for (lang in langs) {

  print(paste0("Fitting model for ", lang))
  
  df = read.csv(paste0("./cleaned_data/l1/", lang, "_clean_data.csv")) %>%
    filter(context == "long")
    
  for (psychometric in psychometrics) {
    
    regression_forms = c(
      paste0(psychometric, " ~ surp + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ ent + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ surp + prev_ent + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ surp + prev_surp + prev2_ent + freq*len + prev_freq*prev_len + prev2_freq*prev2_len")
    )
    
    loglik_df = data.frame(names=regression_names, forms=regression_forms) %>%
      mutate(logliks = map(regression_forms, model_cross_val, df=df, d_var=psychometric )) %>%
      dplyr::select(-forms)
  
  dlls = list()
  
  dll_df = data.frame()
  for (i in regression_names){
      ll1 = loglik_df[loglik_df["names"] == "bl"][2][[1]]
      ll2 = loglik_df[loglik_df["names"] == i][2][[1]]
      dll = ll1 - ll2
      dll = dll[!is.na(dll)]
      ttest = t.test(dll)
      dll_df = rbind(dll_df, data.frame(comp = i, mean = mean(dll), 
                                        upper = mean(dll) + (1.96 * std.error(dll)), lower = mean(dll) - (1.96 * std.error(dll)),
                                        ttest_pval = ttest$p.value, ttest_est = ttest$estimate))
  }
  dll_xlang_ent_df = rbind(dll_xlang_ent_df, dll_df %>% mutate(lang = lang, psychometric = psychometric))
  }
  
}

```

Plotting for these results

```{r}
dll_xlang_ent_df %>%
  filter(comp == "0" & ttest_est > 0)

dll_xlang_ent_df %>%
  mutate(comp = factor(comp, levels = c("2", "1", "0"))) %>%
  mutate(mean = -mean, upper = -upper, lower = -lower) %>%
  mutate(comp = case_when(comp == "0" ~ "w0", comp == "1" ~ "w-1", comp == "2" ~ "w-2")) %>%
  rename(target = comp) %>%
  filter(target != "bl") %>%
  #filter(psychometric == "gaze_rt") %>%
  mutate(sig = case_when( ttest_pval >= 0.05 & ttest_est > 0 ~ " ",
                          ttest_pval < 0.05 & ttest_pval >= 0.01 & ttest_est > 0 ~ "*",
                          ttest_pval < 0.01 & ttest_pval >= 0.001 & ttest_est > 0 ~ "**",
                          ttest_pval < 0.001 & ttest_est > 0 ~ "***")) %>%
  mutate(psychometric = case_when(psychometric == "firstfix_rt" ~ "First Fixation", 
                                  psychometric == "gaze_rt" ~ "Gaze Duration", 
                                  psychometric == "total_rt" ~ "Total Fixation")) %>%
  ggplot(aes(x = target, y = mean, color = target)) +
    geom_hline(yintercept=0, color="blue", linetype="dashed", alpha =0.5) +
    geom_point(position = position_dodge(width = 0.6)) +
    geom_text(aes(y = 0.025, label = sig, color = target), size = 3) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.1, position = position_dodge(width = 0.6)) +
    ylab("ΔLogLiklihood (per word)") + 
    #xlab("Language") +
    scale_x_discrete(labels = c(bquote(w[t-2]), bquote(w[t-1]), bquote(w[t]))) +
    scale_color_manual(values = c("#a1dab4", "#41b6c4", "#225ea8")) +
    #ggtitle("Replacing Surprisal w/ Entropy") +
    facet_grid(psychometric~lang) +
  theme(
    legend.position = "none",
    axis.title.x = element_blank()
  )

#ggsave("./images/dll_ent_replace_gaze.png", width = 8.5, height = 2)
ggsave("./images/l1/dll_ent_replace_all.png", width = 9.2, height = 4)


```

## Add Entropy

Compare baseline model to models where entropy has been added as an additional predictor for each slot. "bl" = baseline with just surprisal "0" = model where entropy has been added at the current word.

```{r}

regression_names = c("bl", "0", "1", "2")

dll_xlang_add_df = data.frame()
for (lang in langs) {

  print(paste0("Fitting model for ", lang))
  df = read.csv(paste0("./cleaned_data/l1/", lang, "_clean_data.csv")) %>%
    filter(context == "long")
  
  for (psychometric in psychometrics) {
    
    regression_forms = c(
      paste0(psychometric, " ~ surp + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ surp + ent + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ surp + prev_surp + prev_ent + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ surp + prev_surp + prev2_surp + prev2_ent + freq*len + prev_freq*prev_len + prev2_freq*prev2_len")
    )
  
  
  loglik_df = data.frame(names=regression_names, forms=regression_forms) %>%
    mutate(logliks = map(regression_forms, model_cross_val, df=df, d_var=psychometric )) %>%
    dplyr::select(-forms)
  
  dlls = list()
  
  dll_df = data.frame()
  for (i in regression_names){
      ll1 = loglik_df[loglik_df["names"] == "bl"][2][[1]]
      ll2 = loglik_df[loglik_df["names"] == i][2][[1]]
      dll = ll1 - ll2
      dll = dll[!is.na(dll)]
      ttest = t.test(dll)
      dll_df = rbind(dll_df, data.frame(comp = i, mean = mean(dll), 
                                        upper = mean(dll) + (1.96 * std.error(dll)), lower = mean(dll) - (1.96 * std.error(dll)),
                                        ttest_pval = ttest$p.value, ttest_est = ttest$estimate))
  }
  dll_xlang_add_df = rbind(dll_xlang_add_df, dll_df %>% mutate(lang = lang, psychometric = psychometric))
  }
  
}

```

```{r}

dll_xlang_add_df %>%
  filter(comp == "0" & ttest_est < 0)

dll_xlang_add_df %>%
  mutate(comp = factor(comp, levels = c("2", "1", "0"))) %>%
  mutate(mean = -mean, upper = -upper, lower = -lower) %>%
  mutate(comp = case_when(comp == "0" ~ "w0", comp == "1" ~ "w-1", comp == "2" ~ "w-2")) %>%
  rename(target = comp) %>%
  filter(target != "bl") %>%
  #filter(psychometric == "gaze_rt") %>%
  mutate(sig = case_when( ttest_pval >= 0.05 & ttest_est > 0~ " ",
                          ttest_pval < 0.05 & ttest_pval >= 0.01 ~ "*",
                          ttest_pval < 0.01 & ttest_pval >= 0.001  ~ "**",
                          ttest_pval < 0.001 ~ "***")) %>%
  mutate(psychometric = case_when(psychometric == "firstfix_rt" ~ "First Fixation", 
                                  psychometric == "gaze_rt" ~ "Gaze Duration", 
                                  psychometric == "total_rt" ~ "Total Fixation")) %>%
  ggplot(aes(x = target, y = mean, color = target)) +
    geom_hline(yintercept=0, color="blue", linetype="dashed", alpha =0.5) +
    geom_point(position = position_dodge(width = 0.6)) +
    geom_text(aes(y = 0.025, label = sig, color = target), size = 3) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.1, position = position_dodge(width = 0.6)) +
    ylab("ΔLogLiklihood (per word)") + 
    xlab("Language") +
    scale_x_discrete(labels = c(bquote(w[t-2]), bquote(w[t-1]), bquote(w[t]))) +
    scale_color_manual(values = c("#a1dab4", "#41b6c4", "#225ea8")) +
    facet_grid(psychometric~lang) +
  theme(
    legend.position = "none",
    axis.title.x = element_blank()
  )

#ggsave("./images/dll_ent_add_gaze.png", width = 8.5, height = 2)
ggsave("./images/l1/dll_ent_add_all.png", width = 9.2, height = 4)


```




## Compare DLL to Linguistic Exposure

```{r}

lang_exposure = data.frame( lang = c("du", "en", "fi", "ge", "gr", "he", "it", "ko", "ru", "sp", "tr"  ),
                            family = c("Indo-European", "Indo-European", "Uralic", "Indo-European", "Indo-European", "Semetic", "Indo-European", "Koreanic", "Indo-European", "Indo-European", "Turkic"),
                            ntoks = c(15.96, 41.21, 2.99, 40.37, 3.61, 0.69, 32.94, 6.28, 39.39, 24.45, 22.52 ))

dll_df = dll_xlang_surp_df %>%
  filter(psychometric == "gaze_rt") %>%
  merge(lang_exposure, by = "lang") %>%
  filter(comp == "0") %>%
  dplyr::select(lang, mean, ntoks, family) %>%
  mutate(measure = "Language")

dll_df_fam = dll_df %>%
  group_by(family) %>%
    summarise(mean = mean(mean),
              ntoks = mean(ntoks)) %>%
  ungroup() %>%
  mutate(lang = family) %>% #just to merge them
  mutate(measure = "Language Family") 

dll_df_plot = rbind(dll_df, dll_df_fam)

```

```{r}

cor.test(dll_df$mean, dll_df$ntoks)
cor.test(dll_df_fam$mean, dll_df_fam$ntoks)

dll_df_plot %>%
  ggplot(aes(x=ntoks, y = mean, label = lang, color = measure)) +
  geom_smooth(method = "lm") +
  geom_point(size = 2) +
  #geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.3) +
  geom_label_repel() +
  ylab("ΔLogLiklihood (per word)") + 
  xlab("# of Training Tokens (Billion)") +
  facet_wrap(~measure, scales = "free_x") +
  scale_color_manual(values = c("#41b6c4", "#225ea8")) +
  theme(
    legend.position = "none"
  )

ggsave("./images/dll_vs_training-size.png", width = 4, height = 4)


```
## Compare DLL to Linguistic Exposure

```{r}

lang_ppl = data.frame( lang = c("du", "en", "fi", "ge", "gr", "he", "it", "ko", "ru", "sp", "tr"  ),
                       family = c("Indo-European", "Indo-European", "Uralic", "Indo-European", "Indo-European", "Semetic", "Indo-European", "Koreanic", "Indo-European", "Indo-European", "Turkic"),
                            ppl = c(8.78, 16.40, 15.05, 10.88, 7.56, 11.01, 10.53,  10.92, 9.15, 12.93, 9.79 ))

lang_ppl_df = dll_xlang_surp_df %>%
  filter(psychometric == "gaze_rt") %>%
  merge(lang_ppl, by = "lang") %>%
  filter(comp == "0") %>%
  dplyr::select(lang, mean, ppl, family) %>%
  mutate(measure = "Language")

dll_ppl_fam = lang_ppl_df %>%
  group_by(family) %>%
    summarise(mean = mean(mean),
              ppl = mean(ppl)) %>%
  ungroup() %>%
  mutate(lang = family) %>% #just to merge them
  mutate(measure = "Language Family") 

dll_ppl_plot = rbind(lang_ppl_df, dll_ppl_fam)

```

```{r}

cor.test(lang_ppl_df$mean, lang_ppl_df$ppl)
cor.test(dll_ppl_fam$mean, dll_ppl_fam$ppl)

dll_ppl_plot %>%
  ggplot(aes(x=ppl, y = mean, label = lang, color = measure)) +
  geom_smooth(method = "lm") +
  geom_point(size = 2) +
  #geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.3) +
  geom_label_repel() +
  ylab("ΔLogLiklihood (per word)") + 
  xlab("# of Training Tokens (Billion)") +
  facet_wrap(~measure, scales = "free_x") +
  scale_color_manual(values = c("#41b6c4", "#225ea8")) +
  theme(
    legend.position = "none"
  )

ggsave("./images/dll_vs_ppl.png", width = 4, height = 4)


```


## Shape of surprisal / RT relationship

```{r}

fit_gam_inner = function(bootstrap_sample, mean_predictors, is_linear) {
  
  df = bootstrap_sample$data
  weights = tabulate(as.integer(bootstrap_sample), nrow(df))
  
  if (is_linear) {
     
    m = gam(psychometric ~ surp + prev_surp + te(freq, len, bs = 'cr') + te(prev_freq, prev_len, bs = 'cr'), data = df, weights = weights)
    terms_to_predict = c("surp", "prev_surp")
  } else {
    m = gam(psychometric ~ s(surp, bs = 'cr', k = 6) + s(prev_surp, bs = 'cr', k = 6) + te(freq, len, bs = 'cr') + te(prev_freq, prev_len, bs = 'cr'), data = df, weights = weights)
    terms_to_predict = c("s(surp)", "s(prev_surp)")

  }
  
  # For fitting with smooths for by-subject effects
  # m = gam(psychometric ~ s(surp, bs='tp', k=6) + s(uniform_id, surp, bs='fs', m=1, k=6) + te(freq, len) + s(prev_surp, bs='tp', k=6) + s(uniform_id, prev_surp, bs='fs', m=1, k=6) + te(prev_freq, prev_len), data = df, weights = weights)

  newdata = data.frame(surp=seq(0,20,by=0.1),
                       prev_surp=mean_predictors$surp,
                       freq=mean_predictors$freq, prev_freq=mean_predictors$freq,
                       len=mean_predictors$freq, prev_len=mean_predictors$freq)
  
  # Returns a matrix N_samples * N_terms.
  per_term_predictions = predict(m, newdata=newdata, terms=terms_to_predict, type="terms")

  # Additive model -- sum across predictor response contributions (matrix columns).
  predictions = rowSums(per_term_predictions)

  return(newdata %>% mutate(y=predictions))
}

fit_gam = function(df, mean_predictors, is_linear, alpha=0.05) {
  # Bootstrap-resample data
  boot_models = df %>% bootstraps(times=10) %>% 
   # Fit a GAM and get predictions for each sample
    mutate(smoothed=map(splits, fit_gam_inner, mean_predictors=mean_predictors, is_linear = is_linear))
  
  # Extract mean and 5% and 95% percentile y-values for each surprisal value
  result = boot_models %>% 
    unnest(smoothed) %>% 
    dplyr::select(surp, y) %>% 
    group_by(surp) %>% 
      summarise(y_lower=quantile(y, alpha / 2), 
                y_upper=quantile(y, 1 - alpha / 2),
                y=mean(y)) %>% 
    ungroup()
  
  return (result)
}

```





Get linear + non-linear gam smooths for each of our languages

```{r}

langs = c("du", "en", "fi", "ge", "gr", "he", "it", "ko", "sp", "tr", "ru")


xlang_linear_smooths_long = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>% filter(context == "long") %>% rename(psychometric = gaze_rt)
  mean_predictors = merged_df %>% summarise(surp = mean(surp), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=T)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_linear_smooths_long = rbind(xlang_linear_smooths_long, gam_smooths %>% mutate(lang = lang, context = "long", linear = "linear"))
}

xlang_nonlinear_smooths_long = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>% filter(context == "long") %>% rename(psychometric = gaze_rt)
  mean_predictors = merged_df %>% summarise(surp = mean(surp), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=F)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_nonlinear_smooths_long = rbind(xlang_nonlinear_smooths_long, gam_smooths %>% mutate(lang = lang, context = "long", linear = "non-linear" ))
}

xlang_linear_smooths_short = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>% filter(context == "short") %>% rename(psychometric = gaze_rt)
  mean_predictors = merged_df %>% summarise(surp = mean(surp), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=T)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_linear_smooths_short = rbind(xlang_linear_smooths_short, gam_smooths %>% mutate(lang = lang, context = "short", linear = "linear"))
}

xlang_nonlinear_smooths_short = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>% filter(context == "short") %>% rename(psychometric = gaze_rt)
  mean_predictors = merged_df %>% summarise(surp = mean(surp), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=F)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_nonlinear_smooths_short = rbind(xlang_nonlinear_smooths_short, gam_smooths %>% mutate(lang = lang, context = "short", linear = "non-linear"))
}

```

```{r}

write.csv(xlang_nonlinear_smooths_short, "./gam_saves/nonlinear_short.csv")
write.csv(xlang_linear_smooths_short, "./gam_saves/linear_short.csv")
write.csv(xlang_nonlinear_smooths_long, "./gam_saves/nonlinear_long.csv")
write.csv(xlang_linear_smooths_long, "./gam_saves/linear_long.csv")

```

## Density Data

```{r}

get_d_points = function(df) {
    x = density(df$surp)$x
    y = density(df$surp)$y
    return(data.frame(x, y))
  }

density_data_short = data.frame()
for(lang in langs) {
  merged_df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>% filter(context == "short")
  density_data = merged_df %>%
    do({get_d_points(.)}) %>%
    filter(x>0, x<20)
  density_data_short = rbind(density_data_short, density_data %>% mutate(lang = lang, context = "short"))
}

density_data_long = data.frame()
for(lang in langs) {
  merged_df = read.csv(paste0("./cleaned_data/", lang, "_clean_data.csv")) %>% filter(context == "long")
  density_data = merged_df %>%
    do({get_d_points(.)}) %>%
    filter(x>0, x<20)
density_data_long = rbind(density_data_long, density_data %>% mutate(lang = lang, context = "long"))

}
 

```

## Plot Surprisal / RT relationship for Short & Long Contexts

```{r}
xlang_nonlinear_smooths_short = read.csv( "./gam_saves/nonlinear_short.csv")
xlang_linear_smooths_short = read.csv( "./gam_saves/linear_short.csv")
xlang_nonlinear_smooths_long = read.csv( "./gam_saves/nonlinear_long.csv")
xlang_linear_smooths_long = read.csv(  "./gam_saves/linear_long.csv")

```


```{r}

# Surprisal curves for long context
  ggplot() +
      annotate("rect", xmin=0, xmax=20, ymin=-18,ymax=-13, fill="blue", alpha=0.05) +
      geom_line(data = density_data_long, aes(x=x, y=y*50 - 18), color="grey") +
      geom_line(data = xlang_linear_smooths_long, aes(x=surp, y=y, color = linear), size=0.5) +
      geom_line(data = xlang_nonlinear_smooths_long, aes(x=surp, y=y, color = linear), size=0.5) +
      geom_ribbon(data = xlang_nonlinear_smooths_long, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      geom_ribbon(data = xlang_linear_smooths_long, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      scale_x_continuous(labels=c(0, 10, 20), breaks=c(0, 10, 20), minor_breaks = NULL) +
      facet_wrap(~lang, nrow = 1) +
      ylab("Slowdown due to surprisal (ms)") +
      xlab("Surprisal of Word") +
      scale_color_manual(values = c("#b7b7b7", "#29bc8b")) +
      scale_fill_manual(values = c("#b7b7b7", "#29bc8b")) +
      scale_linetype_manual(values = c("a", "b")) +
      #ggtitle("Effect of Surprisal on Reading Time across Languages \n Long Context Window")
  theme(
    legend.position = "none"
  )
  
  ggsave("./images/surp_mgpt_long.png", height = 2.4, width = 8)


```


```{r}

# Surprisal curves for short context
  ggplot() +
      annotate("rect", xmin=0, xmax=20, ymin=-18,ymax=-13, fill="blue", alpha=0.05) +
      geom_line(data = density_data_short, aes(x=x, y=y*50 - 18), color="grey") +
      geom_line(data = xlang_linear_smooths_short, aes(x=surp, y=y, color = linear), size=0.5) +
      geom_line(data = xlang_nonlinear_smooths_short, aes(x=surp, y=y, color = linear), size=0.5) +
      geom_ribbon(data = xlang_nonlinear_smooths_short, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      geom_ribbon(data = xlang_linear_smooths_short, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      scale_x_continuous(labels=c(0, 10, 20), breaks=c(0, 10, 20), minor_breaks = NULL) +
            facet_wrap(~lang, nrow = 1) +
      ylab("Slowdown due to surprisal (ms)") +
      xlab("Surprisal of Word") +
      scale_color_manual(values = c("#b7b7b7", "#29bc8b")) +
      scale_fill_manual(values = c("#b7b7b7", "#29bc8b")) +
      scale_linetype_manual(values = c("a", "b")) +
      #ggtitle("Effect of Surprisal on Reading Time across Languages \n Long Context Window")
  theme(
    legend.position = "none"
  )
  ggsave("./images/surp_mgpt_short.png", height = 2.4, width = 8)

```


## Hoover et al. (2022) "Superlinearity" Metric

```{r}

non_linear_delta_df = data.frame()

for(lang in langs) {
for(ctext in contexts) {
  
  df = read.csv(paste0("./cleaned_data/l1/", lang, "_clean_data.csv")) %>% filter(context == ctext) %>% rename(psychometric = gaze_rt)
  
  median_surp = median(df$surp)
  
  high_surp = df %>% filter(surp >= median_surp)
  low_surp = df %>% filter(surp < median_surp)
  
  high_lm = lm(psychometric ~ surp + len + freq + prev_surp + prev_len + prev_freq, data =  high_surp)
  b_high = high_lm$coefficients[2]

  summary(high_lm)
  low_lm = lm(psychometric ~ surp + len + freq + prev_surp + prev_len + prev_freq, data =  low_surp)
  b_low = low_lm$coefficients[2]
  
  delta = b_high - b_low
  delta_df = data_frame(delta = delta, lang = lang, context=ctext)
  
  non_linear_delta_df = rbind(non_linear_delta_df, delta_df)
  
}
}


```

Plot the superlinearity metric by context


```{r}
lang_ppl = data.frame( lang = c("du", "en", "fi", "ge", "gr", "he", "it", "ko", "ru", "sp", "tr"  ),
                            ppl = c(8.78, 16.40, 15.05, 10.88, 7.56, 11.01, 10.53,  10.92, 9.15, 12.93, 9.79 ))

linearity_df = non_linear_delta_df %>%
  merge(lang_ppl, by = c("lang"))

linearity_df %>%
  mutate(context = if_else(context == "short", "Short Context", "Long Context")) %>%
  ggplot(aes(x = ppl, y = delta)) +
  stat_smooth(method = "lm") +
  geom_hline(aes(yintercept = 0), color = "orange", linetype = "dashed") +
  geom_point() +
  geom_label_repel(aes(label = lang)) +
  facet_grid(context~.)+
  ylab("Superlinearity") +
  xlab("Perplexity")
  
  
ggsave("./images/superlinearity.png", width = 3, height = 5)


```


## Entropy!

```{r}

fit_gam_inner = function(bootstrap_sample, mean_predictors, is_linear) {
  
  df = bootstrap_sample$data
  weights = tabulate(as.integer(bootstrap_sample), nrow(df))
  
  if (is_linear) {
    m = gam(psychometric ~ ent + prev_ent + te(freq, len, bs = 'cr') + te(prev_freq, prev_len, bs = 'cr'), data = df, weights = weights)
    terms_to_predict = c("ent", "prev_ent")
    
  } else {
    m = gam(psychometric ~ s(ent, bs = 'cr', k = 6) + s(prev_ent, bs = 'cr', k = 6) + te(freq, len, bs = 'cr') + te(prev_freq, prev_len, bs = 'cr'), data = df, weights = weights)
    terms_to_predict = c("s(ent)", "s(prev_ent)")
  }
  
  newdata = data.frame(ent=seq(0,20,by=0.1),
                       prev_ent=mean_predictors$ent,
                       freq=mean_predictors$freq, prev_freq=mean_predictors$freq,
                       len=mean_predictors$freq, prev_len=mean_predictors$freq) 
  
  # Returns a matrix N_samples * N_terms.
  per_term_predictions = predict(m, newdata=newdata, terms=terms_to_predict, type="terms")

  # Additive model -- sum across predictor response contributions (matrix columns).
  predictions = rowSums(per_term_predictions)

  return(newdata %>% mutate(y=predictions))
}

fit_gam = function(df, mean_predictors, is_linear, alpha=0.05) {
  # Bootstrap-resample data
  boot_models = df %>% bootstraps(times=10) %>% 
   # Fit a GAM and get predictions for each sample
    mutate(smoothed=map(splits, fit_gam_inner, mean_predictors=mean_predictors, is_linear = is_linear))
  
  # Extract mean and 5% and 95% percentile y-values for each surprisal value
  result = boot_models %>% 
    unnest(smoothed) %>% 
    dplyr::select(ent, y) %>% 
    group_by(ent) %>% 
      summarise(y_lower=quantile(y, alpha / 2), 
                y_upper=quantile(y, 1 - alpha / 2),
                y=mean(y)) %>% 
    ungroup()
  
  return (result)
}

```


```{r}

get_d_points = function(df) {
    x = density(df$ent)$x
    y = density(df$ent)$y
    return(data.frame(x, y))
  }

density_data_short = data.frame()
for(lang in langs) {
  merged_df = read.csv(paste0("./cleaned_data/l1/", lang, "_clean_data.csv")) %>% filter(context == "short")
  density_data = merged_df %>%
    do({get_d_points(.)}) %>%
    filter(x>0, x<20)
  density_data_short = rbind(density_data_short, density_data %>% mutate(lang = lang, context = "short"))
}

density_data_long = data.frame()
for(lang in langs) {
  merged_df = read.csv(paste0("./cleaned_data/l1/", lang, "_clean_data.csv")) %>% filter(context == "long")
  density_data = merged_df %>%
    do({get_d_points(.)}) %>%
    filter(x>0, x<20)
density_data_long = rbind(density_data_long, density_data %>% mutate(lang = lang, context = "long"))

}
 

```

```{r}
xlang_nonlinear_smooths_long_ent = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/l1/", lang, "_clean_data.csv")) %>% filter(context == "long") %>% rename(psychometric = gaze_rt)
  mean_predictors = merged_df %>% summarise(ent = mean(ent), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=F)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_nonlinear_smooths_long_ent = rbind(xlang_nonlinear_smooths_long_ent, gam_smooths %>% mutate(lang = lang, context = "long", linear = "non-linear"))
}

xlang_linear_smooths_long_ent = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/l1/", lang, "_clean_data.csv")) %>% filter(context == "long") %>% rename(psychometric = gaze_rt)
  mean_predictors = merged_df %>% summarise(ent = mean(ent), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=T)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_linear_smooths_long_ent = rbind(xlang_linear_smooths_long_ent, gam_smooths %>% mutate(lang = lang, context = "long", linear = "linear"))
}

xlang_nonlinear_smooths_short_ent = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/l1/", lang, "_clean_data.csv")) %>% filter(context == "long") %>% rename(psychometric = gaze_rt)
  mean_predictors = merged_df %>% summarise(ent = mean(ent), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=F)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_nonlinear_smooths_short_ent = rbind(xlang_nonlinear_smooths_short_ent, gam_smooths %>% mutate(lang = lang, context = "short", linear = "non-linear"))
}

xlang_linear_smooths_short_ent = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/l1/", lang, "_clean_data.csv")) %>% filter(context == "long") %>% rename(psychometric = gaze_rt)
  mean_predictors = merged_df %>% summarise(ent = mean(ent), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=T)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_linear_smooths_short_ent = rbind(xlang_linear_smooths_short_ent, gam_smooths %>% mutate(lang = lang, context = "short", linear = "linear"))
}
```

```{r}

# Entropy curves for long contexts
  ggplot() +
      annotate("rect", xmin=0, xmax=20, ymin=-18,ymax=-10, fill="blue", alpha=0.05) +
      geom_line(data = density_data_short, aes(x=x, y=y*50 - 18), color="grey") +
      geom_line(data = xlang_linear_smooths_long_ent, aes(x=ent, y=y, color = linear), size=0.5) +
      geom_line(data = xlang_nonlinear_smooths_long_ent, aes(x=ent, y=y, color = linear), size=0.5) +
      geom_ribbon(data = xlang_nonlinear_smooths_long_ent, aes(x=ent, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      geom_ribbon(data = xlang_linear_smooths_long_ent, aes(x=ent, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      scale_x_continuous(labels=c(0, 10, 20), breaks=c(0, 10, 20), minor_breaks = NULL) +
            facet_wrap(~lang, nrow = 1) +
      ylab("Slowdown due to Entropy (ms)") +
      xlab("Entropy of Word") +
      scale_color_manual(values = c("#b7b7b7", "#29bc8b")) +
      scale_fill_manual(values = c("#b7b7b7", "#29bc8b")) + 
      scale_linetype_manual(values = c("a", "b")) +
      #ggtitle("Effect of Surprisal on Reading Time across Languages \n Long Context Window")
  theme(
    legend.position = "none"
  )
  ggsave("./images/l1/link_ent_long.png", height = 2.4, width = 8)

```

```{r}

# Entropy curves for short contexts
  ggplot() +
      annotate("rect", xmin=0, xmax=20, ymin=-18,ymax=-10, fill="blue", alpha=0.05) +
      geom_line(data = density_data_short, aes(x=x, y=y*50 - 18), color="grey") +
      geom_line(data = xlang_linear_smooths_short_ent, aes(x=ent, y=y, color = linear), size=0.5) +
      geom_line(data = xlang_nonlinear_smooths_short_ent, aes(x=ent, y=y, color = linear), size=0.5) +
      geom_ribbon(data = xlang_nonlinear_smooths_short_ent, aes(x=ent, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      geom_ribbon(data = xlang_linear_smooths_short_ent, aes(x=ent, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      scale_x_continuous(labels=c(0, 10, 20), breaks=c(0, 10, 20), minor_breaks = NULL) +
            facet_wrap(~lang, nrow = 1) +
      ylab("Slowdown due to Entropy (ms)") +
      xlab("Entropy of Word") +
      scale_color_manual(values = c("#b7b7b7", "#29bc8b")) +
      scale_fill_manual(values = c("#b7b7b7", "#29bc8b")) + 
      scale_linetype_manual(values = c("a", "b")) +
      #ggtitle("Effect of Surprisal on Reading Time across Languages \n Long Context Window")
  theme(
    legend.position = "none"
  )
  ggsave("./images/l1/link_ent_short.png", height = 2.4, width = 8)

```