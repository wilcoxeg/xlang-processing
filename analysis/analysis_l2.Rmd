---
title: "Analysis for x linguistic processing -- l2 results"
output: html_notebook
---

```{r}
shhh <- suppressPackageStartupMessages # It's a library, so shhh!

shhh(library( mgcv ))
shhh(library(dplyr))
shhh(library(ggplot2))
shhh(library(lme4))
shhh(library(tidymv))
shhh(library(gamlss))
shhh(library(gsubfn))
shhh(library(lmerTest))
shhh(library(tidyverse))
shhh(library(boot))
shhh(library(rsample))
shhh(library(plotrix))
shhh(library(ggrepel))
shhh(library(mgcv))

theme_set(theme_bw())
options(digits=4)
options(dplyr.summarise.inform = FALSE)
```

```{r}

set.seed(444)
langs = c("du", "fi", "ge", "gr", "he", "it", "sp", "tr", "ru")
psychometrics = c("total_rt", "gaze_rt", "firstfix_rt")
contexts = c("short", "long")

```

## Compute DLL

```{r}

model_cross_val = function(form, df, d_var, num_folds=10){
  
  folds <- cut(seq(1,nrow(df)),breaks=num_folds,labels=FALSE)
  
  estimates <- c()
  models <- c()
  for(i in 1:num_folds){
    testIndexes = which(folds==i,arr.ind=TRUE)
    testData = df[testIndexes,]
    trainData = df[-testIndexes,]

    model = lm(as.formula(form), data = trainData)

    stdev = sigma(model)
    densities <- log(dnorm(testData[[d_var]],
                          mean=predict(model, newdata=testData),
                          sd=stdev))

    estimates <- c(estimates, densities)
  }

  return(estimates)
}

```


## Baseline Investigation of Surprisal

Compare model with surprisal on w, w - 1, and w -2 to models where surprisal has been dropped for each slot.

```{r}

regression_names = c("bl", "0", "1", "2")
# "bl" = baseline model with full surprisals, 0 = surprisal dropped at slot 0 i.e. the current word


dll_xlang_surp_df = data.frame()
for (lang in langs) {

  print(paste0("Fitting model for ", lang))
  
  df = read.csv(paste0("./cleaned_data/l2/", lang, "_clean_data.csv")) %>%
    filter(context == "long")
  
  for (psychometric in psychometrics) {
    
    regression_forms = c(
      paste0(psychometric, " ~ surp + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ surp + prev_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len")
    )
    
    loglik_df = data.frame(names=regression_names, forms=regression_forms) %>%
      mutate(logliks = map(regression_forms, model_cross_val, df=df, d_var=psychometric )) %>%
      dplyr::select(-forms)
    
    dlls = list()
    
    dll_df = data.frame()
    for (i in regression_names){
        ll1 = loglik_df[loglik_df["names"] == "bl"][2][[1]]
        ll2 = loglik_df[loglik_df["names"] == i][2][[1]]
        dll = ll1 - ll2
        dll = dll[!is.na(dll)]
        ttest = t.test(dll)
        dll_df = rbind(dll_df, data.frame(comp = i, mean = mean(dll), 
                                          upper = mean(dll) + (1.96 * std.error(dll)), lower = mean(dll) - (1.96 * std.error(dll)),
                                          ttest_pval = ttest$p.value, ttest_est = ttest$estimate))
    }
    dll_xlang_surp_df = rbind(dll_xlang_surp_df, dll_df %>% mutate(lang = lang, psychometric = psychometric))
  }
}

```

Plotting for these results

```{r}

options(scipen=999)


dll_xlang_surp_df %>%
  mutate(comp = factor(comp, levels = c("2", "1", "0"))) %>%
  rename(target = comp) %>%
  filter(target != "bl") %>%
  mutate(sig = if_else(ttest_pval < 0.05, "p<0.05", "N.S.")) %>%
  mutate(psychometric = case_when(psychometric == "firstfix_rt" ~ "First Fixation", 
                                  psychometric == "gaze_rt" ~ "Gaze Duration", 
                                  psychometric == "total_rt" ~ "Total Fixation")) %>%
  ggplot(aes(x = target, y = mean, color = target)) +
    geom_hline(yintercept=0, color="blue", linetype="dashed", alpha =0.5) +
    geom_point(position = position_dodge(width = 0.6)) +
    #geom_text(aes(y = -0.0025, label = sig), color = "black", size = 2) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.1, position = position_dodge(width = 0.6)) +
    ylab("ΔLogLiklihood (per word)") + 
    xlab("") +
    facet_grid(psychometric~lang) +
    scale_x_discrete(labels = c(bquote(w[t-2]), bquote(w[t-1]), bquote(w[t]))) +
    #ggtitle("Contribution of Surprisal to ΔLL") +
  theme(
    legend.position = "none",
    axis.title.x = element_blank()
  )

ggsave("./images/l2/dll_surp.png", width = 9.2, height = 4)


```

## Replace Surprisal w/ Entropy

Here "bl" = baseline with only surprisal and "0" = model where surprisal has been replaced with entropy at the current word

```{r }

dll_xlang_ent_df = data.frame()
for (lang in langs) {

  print(paste0("Fitting model for ", lang))
  
  df = read.csv(paste0("./cleaned_data/l2/", lang, "_clean_data.csv")) %>%
    filter(context == "long")
    
  for (psychometric in psychometrics) {
    
    regression_forms = c(
      paste0(psychometric, " ~ surp + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ ent + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ surp + prev_ent + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ surp + prev_surp + prev2_ent + freq*len + prev_freq*prev_len + prev2_freq*prev2_len")
    )
    
    loglik_df = data.frame(names=regression_names, forms=regression_forms) %>%
      mutate(logliks = map(regression_forms, model_cross_val, df=df, d_var=psychometric )) %>%
      dplyr::select(-forms)
  
  dlls = list()
  
  dll_df = data.frame()
  for (i in regression_names){
      ll1 = loglik_df[loglik_df["names"] == "bl"][2][[1]]
      ll2 = loglik_df[loglik_df["names"] == i][2][[1]]
      dll = ll1 - ll2
      dll = dll[!is.na(dll)]
      ttest = t.test(dll)
      dll_df = rbind(dll_df, data.frame(comp = i, mean = mean(dll), 
                                        upper = mean(dll) + (1.96 * std.error(dll)), lower = mean(dll) - (1.96 * std.error(dll)),
                                        ttest_pval = ttest$p.value, ttest_est = ttest$estimate))
  }
  dll_xlang_ent_df = rbind(dll_xlang_ent_df, dll_df %>% mutate(lang = lang, psychometric = psychometric))
  }
  
}

```

Plotting for these results

```{r}
dll_xlang_ent_df %>%
  filter(comp == "0" & ttest_est > 0)

dll_xlang_ent_df %>%
  mutate(comp = factor(comp, levels = c("2", "1", "0"))) %>%
  mutate(mean = -mean, upper = -upper, lower = -lower) %>%
  mutate(comp = case_when(comp == "0" ~ "w0", comp == "1" ~ "w-1", comp == "2" ~ "w-2")) %>%
  rename(target = comp) %>%
  filter(target != "bl") %>%
  #filter(psychometric == "gaze_rt") %>%
  mutate(sig = if_else(ttest_pval < 0.05, "p<0.05", "N.S.")) %>%
  mutate(psychometric = case_when(psychometric == "firstfix_rt" ~ "First Fixation", 
                                  psychometric == "gaze_rt" ~ "Gaze Duration", 
                                  psychometric == "total_rt" ~ "Total Fixation")) %>%
  ggplot(aes(x = target, y = mean, color = target)) +
    geom_hline(yintercept=0, color="blue", linetype="dashed", alpha =0.5) +
    geom_point(position = position_dodge(width = 0.6)) +
    #geom_text(aes(y = -0.0025, label = sig), color = "black", size = 2) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.1, position = position_dodge(width = 0.6)) +
    ylab("ΔLogLiklihood (per word)") + 
    #xlab("Language") +
    scale_x_discrete(labels = c(bquote(w[t-2]), bquote(w[t-1]), bquote(w[t]))) +
    #ggtitle("Replacing Surprisal w/ Entropy") +
    facet_grid(psychometric~lang) +
  theme(
    legend.position = "none",
    axis.title.x = element_blank()
  )

#ggsave("./images/dll_ent_replace_gaze.png", width = 8.5, height = 2)
ggsave("./images/l2/dll_ent_replace_all.png", width = 9.2, height = 4)


```

## Add Entropy

Compare baseline model to models where entropy has been added as an additional predictor for each slot. "bl" = baseline with just surprisal "0" = model where entropy has been added at the current word.

```{r}


regression_names = c("bl", "0", "1", "2")

dll_xlang_add_df = data.frame()
for (lang in langs) {

  print(paste0("Fitting model for ", lang))
  df = read.csv(paste0("./cleaned_data/l2/", lang, "_clean_data.csv")) %>%
    filter(context == "long")
  
  for (psychometric in psychometrics) {
    
    regression_forms = c(
      paste0(psychometric, " ~ surp + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ surp + ent + prev_surp + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ surp + prev_surp + prev_ent + prev2_surp + freq*len + prev_freq*prev_len + prev2_freq*prev2_len"),
      paste0(psychometric, " ~ surp + prev_surp + prev2_surp + prev2_ent + freq*len + prev_freq*prev_len + prev2_freq*prev2_len")
    )
  
  
  loglik_df = data.frame(names=regression_names, forms=regression_forms) %>%
    mutate(logliks = map(regression_forms, model_cross_val, df=df, d_var=psychometric )) %>%
    dplyr::select(-forms)
  
  dlls = list()
  
  dll_df = data.frame()
  for (i in regression_names){
      ll1 = loglik_df[loglik_df["names"] == "bl"][2][[1]]
      ll2 = loglik_df[loglik_df["names"] == i][2][[1]]
      dll = ll1 - ll2
      dll = dll[!is.na(dll)]
      ttest = t.test(dll)
      dll_df = rbind(dll_df, data.frame(comp = i, mean = mean(dll), 
                                        upper = mean(dll) + (1.96 * std.error(dll)), lower = mean(dll) - (1.96 * std.error(dll)),
                                        ttest_pval = ttest$p.value, ttest_est = ttest$estimate))
  }
  dll_xlang_add_df = rbind(dll_xlang_add_df, dll_df %>% mutate(lang = lang, psychometric = psychometric))
  }
  
}

```

```{r}


dll_xlang_add_df %>%
  mutate(comp = factor(comp, levels = c("2", "1", "0"))) %>%
  mutate(mean = -mean, upper = -upper, lower = -lower) %>%
  mutate(comp = case_when(comp == "0" ~ "w0", comp == "1" ~ "w-1", comp == "2" ~ "w-2")) %>%
  rename(target = comp) %>%
  filter(target != "bl") %>%
  #filter(psychometric == "gaze_rt") %>%
  mutate(sig = if_else(ttest_pval < 0.05, "p<0.05", "N.S.")) %>%
  mutate(psychometric = case_when(psychometric == "firstfix_rt" ~ "First Fixation", 
                                  psychometric == "gaze_rt" ~ "Gaze Duration", 
                                  psychometric == "total_rt" ~ "Total Fixation")) %>%
  ggplot(aes(x = target, y = mean, color = target)) +
    geom_hline(yintercept=0, color="blue", linetype="dashed", alpha =0.5) +
    geom_point(position = position_dodge(width = 0.6)) +
    geom_errorbar(aes(ymin=lower, ymax=upper), width = 0.1, position = position_dodge(width = 0.6)) +
    ylab("ΔLogLiklihood (per word)") + 
    xlab("Language") +
    scale_x_discrete(labels = c(bquote(w[t-2]), bquote(w[t-1]), bquote(w[t]))) +
    facet_grid(psychometric~lang) +
  theme(
    legend.position = "none",
    axis.title.x = element_blank()
  )

#ggsave("./images/dll_ent_add_gaze.png", width = 8.5, height = 2)
ggsave("./images/l2/dll_ent_add_all.png", width = 9.2, height = 4)


```





## Shape of surprisal / RT relationship

```{r}

fit_gam_inner = function(bootstrap_sample, mean_predictors, is_linear) {

  df = bootstrap_sample$data
  weights = tabulate(as.integer(bootstrap_sample), nrow(df))
  
  if (is_linear) {
     
    m = gam(psychometric ~ surp + prev_surp + te(freq, len, bs = 'cr') + te(prev_freq, prev_len, bs = 'cr'), data = df, weights = weights)
    terms_to_predict = c("surp", "prev_surp")
    
  } else {
    m = gam(psychometric ~ s(surp, bs = 'cr', k = 6) + s(prev_surp, bs = 'cr', k = 6) + te(freq, len, bs = 'cr') + te(prev_freq, prev_len, bs = 'cr'), data = df, weights = weights)
    terms_to_predict = c("s(surp)", "s(prev_surp)")

  }
 
  newdata = data.frame(surp=seq(0,20,by=0.1),
                       prev_surp=mean_predictors$surp,
                       freq=mean_predictors$freq, prev_freq=mean_predictors$freq,
                       len=mean_predictors$freq, prev_len=mean_predictors$freq)
  
  # Returns a matrix N_samples * N_terms.
  per_term_predictions = predict(m, newdata=newdata, terms=terms_to_predict, type="terms")

  # Additive model -- sum across predictor response contributions (matrix columns).
  predictions = rowSums(per_term_predictions)

  return(newdata %>% mutate(y=predictions))
}

fit_gam = function(df, mean_predictors, is_linear, alpha=0.05) {
  # Bootstrap-resample data
  boot_models = df %>% bootstraps(times=10) %>% 
   # Fit a GAM and get predictions for each sample
    mutate(smoothed=map(splits, fit_gam_inner, mean_predictors=mean_predictors, is_linear = is_linear))
  
  # Extract mean and 5% and 95% percentile y-values for each surprisal value
  result = boot_models %>% 
    unnest(smoothed) %>% 
    dplyr::select(surp, y) %>% 
    group_by(surp) %>% 
      summarise(y_lower=quantile(y, alpha / 2), 
                y_upper=quantile(y, 1 - alpha / 2),
                y=mean(y)) %>% 
    ungroup()
  
  return (result)
}

```





Get linear + non-linear gam smooths for each of our languages

```{r}

xlang_linear_smooths_long = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/l2/", lang, "_clean_data.csv")) %>% filter(context == "long") %>% rename(psychometric = gaze_rt)
  mean_predictors = merged_df %>% summarise(surp = mean(surp), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=T)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_linear_smooths_long = rbind(xlang_linear_smooths_long, gam_smooths %>% mutate(lang = lang, context = "long", linear = "linear"))
}

xlang_nonlinear_smooths_long = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/l2/", lang, "_clean_data.csv")) %>% filter(context == "long") %>% rename(psychometric = gaze_rt)
  mean_predictors = merged_df %>% summarise(surp = mean(surp), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=F)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_nonlinear_smooths_long = rbind(xlang_nonlinear_smooths_long, gam_smooths %>% mutate(lang = lang, context = "long", linear = "non-linear" ))
}

xlang_linear_smooths_short = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/l2/", lang, "_clean_data.csv")) %>% filter(context == "short") %>% rename(psychometric = gaze_rt)
  mean_predictors = merged_df %>% summarise(surp = mean(surp), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=T)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_linear_smooths_short = rbind(xlang_linear_smooths_short, gam_smooths %>% mutate(lang = lang, context = "short", linear = "linear"))
}

xlang_nonlinear_smooths_short = data.frame()
for (lang in langs) {
  print(paste0("Fitting model for ", lang))
  merged_df = read.csv(paste0("./cleaned_data/l2/", lang, "_clean_data.csv")) %>% filter(context == "short") %>% rename(psychometric = gaze_rt)
  mean_predictors = merged_df %>% summarise(surp = mean(surp), len = mean(len), freq = mean(freq))
  smooths = merged_df %>% fit_gam(., mean_predictors, is_linear=F)
  #Fix 0 surprisal = 0 ms
  gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
  xlang_nonlinear_smooths_short = rbind(xlang_nonlinear_smooths_short, gam_smooths %>% mutate(lang = lang, context = "short", linear = "non-linear"))
}

```



## Density Data

```{r}

get_d_points = function(df) {
    x = density(df$surp)$x
    y = density(df$surp)$y
    return(data.frame(x, y))
  }

density_data_short = data.frame()
for(lang in langs) {
  merged_df = read.csv(paste0("./cleaned_data/l2/", lang, "_clean_data.csv")) %>% filter(context == "short")
  density_data = merged_df %>%
    do({get_d_points(.)}) %>%
    filter(x>0, x<20)
  density_data_short = rbind(density_data_short, density_data %>% mutate(lang = lang, context = "short"))
}

density_data_long = data.frame()
for(lang in langs) {
  merged_df = read.csv(paste0("./cleaned_data/l2/", lang, "_clean_data.csv")) %>% filter(context == "long")
  density_data = merged_df %>%
    do({get_d_points(.)}) %>%
    filter(x>0, x<20)
density_data_long = rbind(density_data_long, density_data %>% mutate(lang = lang, context = "long"))

}
 

```

## Plot Surprisal / RT relationship for Short & Long Contexts

```{r}
xlang_nonlinear_smooths_short = read.csv( "./gam_saves/nonlinear_short.csv")
xlang_linear_smooths_short = read.csv( "./gam_saves/linear_short.csv")
xlang_nonlinear_smooths_long = read.csv( "./gam_saves/nonlinear_long.csv")
xlang_linear_smooths_long = read.csv(  "./gam_saves/linear_long.csv")

```


```{r}

# Surprisal curves for long context
  ggplot() +
      annotate("rect", xmin=0, xmax=20, ymin=-18,ymax=-13, fill="blue", alpha=0.05) +
      geom_line(data = density_data_long, aes(x=x, y=y*50 - 18), color="grey") +
      geom_line(data = xlang_linear_smooths_long, aes(x=surp, y=y, color = linear), size=0.5) +
      geom_line(data = xlang_nonlinear_smooths_long, aes(x=surp, y=y, color = linear), size=0.5) +
      geom_ribbon(data = xlang_nonlinear_smooths_long, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      geom_ribbon(data = xlang_linear_smooths_long, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      scale_x_continuous(labels=c(0, 10, 20), breaks=c(0, 10, 20), minor_breaks = NULL) +
      facet_wrap(~lang, nrow = 1) +
      ylab("Slowdown due to surprisal (ms)") +
      xlab("Surprisal of Word") +
      scale_color_manual(values = c("#b7b7b7", "#29bc8b")) +
      scale_fill_manual(values = c("#b7b7b7", "#29bc8b")) +
      scale_linetype_manual(values = c("a", "b")) +
      #ggtitle("Effect of Surprisal on Reading Time across Languages \n Long Context Window")
  theme(
    legend.position = "none"
  )
  
  ggsave("./images/l2/surp_mgpt_long.png", height = 2.4, width = 8)


```


```{r}

# Surprisal curves for short context
  ggplot() +
      annotate("rect", xmin=0, xmax=20, ymin=-18,ymax=-13, fill="blue", alpha=0.05) +
      geom_line(data = density_data_short, aes(x=x, y=y*50 - 18), color="grey") +
      geom_line(data = xlang_linear_smooths_short, aes(x=surp, y=y, color = linear), size=0.5) +
      geom_line(data = xlang_nonlinear_smooths_short, aes(x=surp, y=y, color = linear), size=0.5) +
      geom_ribbon(data = xlang_nonlinear_smooths_short, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      geom_ribbon(data = xlang_linear_smooths_short, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = linear), alpha=0.3, size=0.5) +
      scale_x_continuous(labels=c(0, 10, 20), breaks=c(0, 10, 20), minor_breaks = NULL) +
            facet_wrap(~lang, nrow = 1) +
      ylab("Slowdown due to surprisal (ms)") +
      xlab("Surprisal of Word") +
      scale_color_manual(values = c("#b7b7b7", "#29bc8b")) +
      scale_fill_manual(values = c("#b7b7b7", "#29bc8b")) +
      scale_linetype_manual(values = c("a", "b")) +
      #ggtitle("Effect of Surprisal on Reading Time across Languages \n Long Context Window")
  theme(
    legend.position = "none"
  )
  ggsave("./images/l2/surp_mgpt_short.png", height = 2.4, width = 8)

```

